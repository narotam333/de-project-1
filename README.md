# de-project-1

Overview:
Running Apache Airflow (using extended image) on docker, creating & executing first DAG and connecting to AWS S3Â bucket

Steps executed:
- Running Airflow on local using Docker Container
- DAG and Tasks creation in Airflow
- Data creation and processing using Python programming
- Data storage on AWS S3 using AWS SDK
- Pipeline Orchestrattion in Airflow 
- Data validation using AWS CLI

Technology Stack used:
Docker, Airflow, Python, AWS SDK , AWS CLI and AWS S3
